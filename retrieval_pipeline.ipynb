{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IJpGHj4ow2DT"},"outputs":[],"source":["!pip install datasets\n","!pip install transformers\n","!pip install -U sentence-transformers\n","!pip install rank_bm25\n","\n","from datasets import load_dataset\n","from datasets import get_dataset_config_names\n","from google.colab import drive\n","from rank_bm25 import BM25Okapi\n","\n","import torch\n","import numpy as np\n","import time\n","\n","# For debugging torch\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQs368Ehw7MX"},"outputs":[],"source":["# file path\n","%cd /content/drive/My Drive/Colab Notebooks/UCL MSc Project/Data\n","\n","### Settings ###\n","#@title Dataset Setting \n","dataset = 'SCAN' #@param [\"SCAN\", \"COGS\"]\n","\n","print(f'Using {dataset} dataset')\n","\n","\n","data = load_dataset('csv', data_files={'train': f\"./{dataset.lower()}_train.csv\", 'test': f\"./{dataset.lower()}_test.csv\"})\n","\n","train = data['train']\n","test = data['test']\n","import random\n","\n","if dataset == 'SCAN':\n","  input = 'commands'\n","  target = 'actions'\n","\n","if dataset == 'COGS':\n","  train = train.select(range(len(train)-1))\n","  input = 'source'\n","  target = 'target'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9egrP0TrxHM6"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList, GPTJForCausalLM\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","##### Select model variant #####\n","#@title Model Setting \n","model_variant = 'gpt-neo-2.7B' #@param [ \"gpt-neo-125M\", \"gpt2-medium\", \"gpt-neo-1.3B\", \"gpt-neo-2.7B\"]\n","print(f\"Running {model_variant}\")\n","\n","if \"gpt2\" in model_variant:\n","  tokenizer = AutoTokenizer.from_pretrained(f\"{model_variant}\")\n","  model = AutoModelForCausalLM.from_pretrained(f\"{model_variant}\")\n","else:\n","  tokenizer = AutoTokenizer.from_pretrained(f\"EleutherAI/{model_variant}\")\n","  model = AutoModelForCausalLM.from_pretrained(f\"EleutherAI/{model_variant}\")\n","\n","model.to(device)\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48bUuHqX0tYN"},"outputs":[],"source":["# Class to stop the model from generating once it encounters the specified tokens\n","class KeywordsStoppingCriteria(StoppingCriteria):\n","    def __init__(self, keywords_ids:list):\n","        self.keywords = keywords_ids\n","\n","    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n","        if input_ids[0][-1] in self.keywords:\n","            return True\n","        return False\n","\n","# Set stop tokens\n","if dataset == 'CFQ':\n","  stop_words = ['$']\n","else:\n","  stop_words = ['\\n']\n","\n","stop_ids = [tokenizer.encode(w)[0] for w in stop_words]\n","stop_criteria = KeywordsStoppingCriteria(stop_ids)\n","the_stop_word = stop_words[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RI3PfFY0Q1vV"},"outputs":[],"source":["# Create few shot prompt or in-context learning - instructions are optional\n","# Examples are obtained from training data and the test prompt is obtained from test as they have to come from different distributions to test the systematic generalization\n","def batch_few_shot_prompt(train_data: list, test_data: list, num_examples: int, examples_selection='random', include_instruction=False):\n","  batch = []\n","\n","  # N/A as we don't use instructions\n","  # TODO: If we need to do instructions to account for other datasets\n","  instruction = f\"Convert commands to actions based on Simplified version of the CommAI Navigation tasks {the_stop_word}\" if include_instruction else ''\n","  \n","  print(f'running {examples_selection}')\n","  # Pre-calculate cosine similarity\n","  if examples_selection == 'nearest':\n","    from sentence_transformers import SentenceTransformer\n","\n","    sentence_model = SentenceTransformer('sentence-transformers/nli-roberta-base-v2')\n","    test_encode = sentence_model.encode(test_data[input])\n","    train_encode = sentence_model.encode(train_data[input])\n","\n","    from scipy.spatial import distance\n","\n","    cosine_similarity = distance.cdist(test_encode, train_encode, metric='cosine')\n","    cosine_similarity_sorted = np.argsort(cosine_similarity, axis=1)[:, :num_examples]\n","\n","    del sentence_model\n","    torch.cuda.empty_cache()\n","  \n","  # Need to train mepr first: get candidates -> score individual -> train\n","  # Change model path accordingly\n","  if examples_selection == 'mepr':\n","    from transformers import BertTokenizer, BertModel\n","    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model_q = BertModel.from_pretrained(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_q_mepr').to(device)\n","    model_p = BertModel.from_pretrained(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_p_mepr').to(device)\n","\n","    stack = []\n","    chunk_size_test = 300\n","    chunk_size_train = 200\n","\n","    for g in range(0, len(test_data), chunk_size_test):\n","      print(f'g is {g}')\n","      for k in range(0, len(train_data), chunk_size_train):\n","        torch.cuda.empty_cache()\n","        with torch.no_grad():\n","          q = model_q(**bert_tokenizer(test_data[g:g+chunk_size_test][input], return_tensors='pt', padding=True).to(device)).last_hidden_state\n","          p = model_p(**bert_tokenizer(train_data[k:k+chunk_size_train][input], return_tensors='pt', padding=True).to(device)).last_hidden_state\n","\n","        # extract CLS since bert\n","        cls_q = q[:,0]\n","        cls_p = p[:,0]\n","\n","        cosine = distance.cdist(cls_q, cls_p, metric='cosine')\n","        cosine_sorted = np.argsort(cosine, axis=1)[:, :num_examples]\n","        stack.append(cosine_sorted)\n","    \n","    mepr_similarity = np.vstack(stack)\n","\n","  # Need to train fpr individual first: get candidates -> score individual -> train\n","  # Change model path accordingly\n","  if examples_selection == 'individual':\n","    from sentence_transformers import SentenceTransformer\n","\n","    model_q = SentenceTransformer(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_q_individual')\n","    model_p = SentenceTransformer(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_p_individual')\n","\n","    stack = []\n","    chunk_size_test = 300\n","    chunk_size_train = 200\n","\n","    for g in range(0, len(test_data), chunk_size_test):\n","      print(f'g is {g}')\n","      for k in range(0, len(train_data), chunk_size_train):\n","        q = model_q.encode(test_data[g:g+chunk_size_test][input])\n","        p = model_p.encoder(train_data[k:k+chunk_size_train][input])\n","\n","        cosine = distance.cdist(q, p, metric='cosine')\n","        cosine_sorted = np.argsort(cosine, axis=1)[:, :num_examples]\n","        stack.append(cosine_sorted)\n","    \n","    individual_similarity = np.vstack(stack)\n","\n","  # Need to train fpr pairwise first: get candidates -> score pair -> train\n","  # Change model path accordingly\n","  if examples_selection == 'pairwise':\n","    from sentence_transformers import SentenceTransformer\n","\n","    model_q = SentenceTransformer(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_q_pairwise')\n","    model_p = SentenceTransformer(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_p_pairwise')\n","\n","    stack = []\n","    chunk_size_test = 300\n","    chunk_size_train = 200\n","\n","    for g in range(0, len(test_data), chunk_size_test):\n","      print(f'g is {g}')\n","      for k in range(0, len(train_data), chunk_size_train):\n","        q = model_q.encode(test_data[g:g+chunk_size_test][input])\n","        p = model_p.encoder(train_data[k:k+chunk_size_train][input])\n","\n","        cosine = distance.cdist(q, p, metric='cosine')\n","        cosine_sorted = np.argsort(cosine, axis=1)[:, :num_examples]\n","        stack.append(cosine_sorted)\n","    \n","    pairwise_similarity = np.vstack(stack)\n","\n","  # Assign examples for each of the test prompt\n","  for i in range(len(test_data)):\n","    print(f'generating test prompt i is {i}')\n","    few_shot_prompt = ''\n","    # Get training examples indices\n","    if examples_selection == 'random':\n","      train_examples_pos = np.random.choice(len(train_data) , num_examples, replace=False)\n","    if examples_selection == 'nearest':\n","      train_examples_pos = cosine_similarity_sorted[i, :]\n","    if examples_selection == 'medpr':\n","      train_examples_pos = mepr_similarity[i,:]\n","    if examples_selection == 'individual':\n","      train_examples_pos = invididual_similarity[i, :]\n","    if examples_selection == 'pairwise':\n","      train_examples_pos = pairwise_similarity[i, :]\n","      \n","\n","    few_shot_prompt += instruction\n","\n","    for j in train_examples_pos.tolist():\n","      few_shot_prompt += (f\"{input}:\" + train_data[j][input] + ' ' + f\"{target}:\" + train_data[j][target] + the_stop_word)\n","\n","    few_shot_prompt += (f\"{input}:\" + test_data[i][input] + ' ' + f\"{target}:\")\n","\n","    batch.append(few_shot_prompt)\n","  \n","  # Sanity check\n","  print(\"++++++++++++++++++++++++++\")\n","  print(\"++++++ Sanity Check ++++++\")\n","  print(batch[np.random.randint(0, len(test_data))])\n","  print(\"++++++++++++++++++++++++++\")\n","  print(\"                          \")\n","\n","  return batch "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nf4OHdrQrvg"},"outputs":[],"source":["##### SETTINGS #####\n","#@title Search Hyperparameters\n","# number of examples\n","k =  1 #@param {type:\"integer\"}\n","# Don't change batch size\n","batch_size = 1\n","# Max context window\n","if 'gpt2' in model_variant:\n","  max_length = 1024\n","else:\n","  max_length = 2048\n","\n","search_method = 'random' #@param [\"random\", \"nearest\", \"mepr\", \"individual\", \"pairwise\"]\n","##### SETTINGS #####\n","\n","# print(f'Running {model} with {dataset} using {search_method}')\n","\n","few_shot = batch_few_shot_prompt(train, test, num_examples=k, \n","                                 examples_selection=search_method)\n","\n","# Padding has to be on the left for proper text generation because the model uses rightmost token to predict the next token \n","# So padding would result in weird predictions\n","tokenizer.padding_side = \"left\"\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Keep track of how many we actually evaluate with how much is matched.\n","match = 0\n","total = 0\n","\n","# Output storage\n","gold_storage = []\n","generated_storage = []\n","predicted_storage = []\n","\n","for i in range(0, len(test), batch_size):\n","  with torch.no_grad():\n","    few_shot_encoded = tokenizer(few_shot[i:i+batch_size], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length - len(tokenizer(test[i][target])))\n","    if few_shot_encoded.input_ids.shape[1] < (max_length - len(tokenizer(test[i][target]))):\n","      few_shot_encoded = few_shot_encoded.to(device)\n","      total += batch_size\n","    else:\n","      print(\"#############################\")\n","      print(\"####### Batch Skipped #######\")\n","      print(\"#############################\")\n","      print(\"                             \")\n","\n","      continue\n","    \n","    # GPT max output length is 2048\n","    # Beam search results in out of memory error - future work could be to test beam search if more resources are available\n","\n","    start = time.process_time()\n","\n","    test_decode = model.generate(**few_shot_encoded,\n","                                 stopping_criteria=StoppingCriteriaList([stop_criteria]),\n","                                 temperature=0,\n","                                 max_length=max_length)\n","    \n","    end = time.process_time()\n","    \n","    generated_texts = tokenizer.batch_decode(test_decode, \n","                                             skip_special_tokens=True)\n","    \n","    generated_storage.append(generated_texts)\n","\n","    for j in range(len(generated_texts)):\n","      cur_predicted = generated_texts[j].replace(tokenizer.decode(tokenizer(few_shot[i+j]).input_ids),\"\") # Discard examples and test prompt\n","      cur_predicted = cur_predicted.split(the_stop_word)[0] # Truncate generated in the case that the stopping criteria does not work properly\n","\n","      cur_gold = tokenizer.decode(tokenizer(test[i+j][target], return_tensors='pt').input_ids[0])\n","\n","      predicted_storage.append(cur_predicted)\n","      gold_storage.append(cur_gold)\n","      \n","\n","      print(\"-------------------------\")\n","      print(\"------- Checking --------\")\n","      print(generated_texts[j]) # Generated text with the examples and test prompt\n","      print(f\"Generated : {cur_predicted}\")\n","      # print(f\"{cur_generated.split()}\")\n","      print(f\"Gold : {cur_gold}\")\n","      # print(f\"{cur_gold.split()}\")\n","      print(f\"Match? : {cur_gold == cur_predicted}\")\n","      print(\"-------------------------\")\n","      print(\"                         \")\n","\n","      if cur_gold == cur_predicted:\n","        match += 1 \n","\n","\n","  if device.type == 'cuda':\n","    print('Current cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n","  print(f\"Inference time for batch size {batch_size} : {end-start} , so {(end-start)/batch_size} each\")\n","  print(f\"Current total match for {k} examples : {match}/{total}\")\n","  print(\"                         \")\n","\n","  torch.cuda.empty_cache() # Clear unused memory to prevent out of memory\n","\n","# Save results\n","generated_storage_array = np.array(generated_storage)\n","gold_storage_array = np.array(gold_storage)\n","predicted_storage_array = np.array(predicted_storage)\n","\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Generated {k} Examples - {search_method}', generated_storage_array)\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Gold {k} Examples - {search_method}', gold_storage_array)\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Predicted {k} Examples - {search_method}', predicted_storage_array)\n"]},{"cell_type":"code","source":["##### SETTINGS #####\n","#@title Search Hyperparameters\n","# number of examples\n","k =  5 #@param {type:\"integer\"}\n","# Don't change batch size\n","batch_size = 1\n","# Max context window\n","if 'gpt2' in model_variant:\n","  max_length = 1024\n","else:\n","  max_length = 2048\n","\n","search_method = search_method\n","##### SETTINGS #####\n","\n","# print(f'Running {model} with {dataset} using {search_method}')\n","\n","few_shot = batch_few_shot_prompt(train, test, num_examples=k, \n","                                 examples_selection=search_method)\n","\n","# Padding has to be on the left for proper text generation because the model uses rightmost token to predict the next token \n","# So padding would result in weird predictions\n","tokenizer.padding_side = \"left\"\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Keep track of how many we actually evaluate with how much is matched.\n","match = 0\n","total = 0\n","\n","# Output storage\n","gold_storage = []\n","generated_storage = []\n","predicted_storage = []\n","\n","for i in range(0, len(test), batch_size):\n","  with torch.no_grad():\n","    few_shot_encoded = tokenizer(few_shot[i:i+batch_size], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length - len(tokenizer(test[i][target])))\n","    if few_shot_encoded.input_ids.shape[1] < (max_length - len(tokenizer(test[i][target]))):\n","      few_shot_encoded = few_shot_encoded.to(device)\n","      total += batch_size\n","    else:\n","      print(\"#############################\")\n","      print(\"####### Batch Skipped #######\")\n","      print(\"#############################\")\n","      print(\"                             \")\n","\n","      continue\n","    \n","    # GPT max output length is 2048\n","    # Beam search results in out of memory error - future work could be to test beam search if more resources are available\n","\n","    start = time.process_time()\n","\n","    test_decode = model.generate(**few_shot_encoded,\n","                                 stopping_criteria=StoppingCriteriaList([stop_criteria]),\n","                                 temperature=0,\n","                                 max_length=max_length)\n","    \n","    end = time.process_time()\n","    \n","    generated_texts = tokenizer.batch_decode(test_decode, \n","                                             skip_special_tokens=True)\n","    \n","    generated_storage.append(generated_texts)\n","\n","    for j in range(len(generated_texts)):\n","      cur_predicted = generated_texts[j].replace(tokenizer.decode(tokenizer(few_shot[i+j]).input_ids),\"\") # Discard examples and test prompt\n","      cur_predicted = cur_predicted.split(the_stop_word)[0] # Truncate generated in the case that the stopping criteria does not work properly\n","\n","      cur_gold = tokenizer.decode(tokenizer(test[i+j][target], return_tensors='pt').input_ids[0])\n","\n","      predicted_storage.append(cur_predicted)\n","      gold_storage.append(cur_gold)\n","      \n","\n","      print(\"-------------------------\")\n","      print(\"------- Checking --------\")\n","      print(generated_texts[j]) # Generated text with the examples and test prompt\n","      print(f\"Generated : {cur_predicted}\")\n","      # print(f\"{cur_generated.split()}\")\n","      print(f\"Gold : {cur_gold}\")\n","      # print(f\"{cur_gold.split()}\")\n","      print(f\"Match? : {cur_gold == cur_predicted}\")\n","      print(\"-------------------------\")\n","      print(\"                         \")\n","\n","      if cur_gold == cur_predicted:\n","        match += 1 \n","\n","\n","  if device.type == 'cuda':\n","    print('Current cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n","  print(f\"Inference time for batch size {batch_size} : {end-start} , so {(end-start)/batch_size} each\")\n","  print(f\"Current total match for {k} examples : {match}/{total}\")\n","  print(\"                         \")\n","\n","  torch.cuda.empty_cache() # Clear unused memory to prevent out of memory\n","\n","# Save results\n","generated_storage_array = np.array(generated_storage)\n","gold_storage_array = np.array(gold_storage)\n","predicted_storage_array = np.array(predicted_storage)\n","\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Generated {k} Examples - {search_method}', generated_storage_array)\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Gold {k} Examples - {search_method}', gold_storage_array)\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Predicted {k} Examples - {search_method}', predicted_storage_array)\n"],"metadata":{"id":"IsMzupWQpnBb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##### SETTINGS #####\n","#@title Search Hyperparameters\n","# number of examples\n","k =  10 #@param {type:\"integer\"}\n","# Don't change batch size\n","batch_size = 1\n","# Max context window\n","if 'gpt2' in model_variant:\n","  max_length = 1024\n","else:\n","  max_length = 2048\n","\n","search_method = search_method\n","##### SETTINGS #####\n","\n","# print(f'Running {model} with {dataset} using {search_method}')\n","\n","few_shot = batch_few_shot_prompt(train, test, num_examples=k, \n","                                 examples_selection=search_method)\n","\n","# Padding has to be on the left for proper text generation because the model uses rightmost token to predict the next token \n","# So padding would result in weird predictions\n","tokenizer.padding_side = \"left\"\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Keep track of how many we actually evaluate with how much is matched.\n","match = 0\n","total = 0\n","\n","# Output storage\n","gold_storage = []\n","generated_storage = []\n","predicted_storage = []\n","\n","for i in range(0, len(test), batch_size):\n","  with torch.no_grad():\n","    few_shot_encoded = tokenizer(few_shot[i:i+batch_size], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length - len(tokenizer(test[i][target])))\n","    if few_shot_encoded.input_ids.shape[1] < (max_length - len(tokenizer(test[i][target]))):\n","      few_shot_encoded = few_shot_encoded.to(device)\n","      total += batch_size\n","    else:\n","      print(\"#############################\")\n","      print(\"####### Batch Skipped #######\")\n","      print(\"#############################\")\n","      print(\"                             \")\n","\n","      continue\n","    \n","    # GPT max output length is 2048\n","    # Beam search results in out of memory error - future work could be to test beam search if more resources are available\n","\n","    start = time.process_time()\n","\n","    test_decode = model.generate(**few_shot_encoded,\n","                                 stopping_criteria=StoppingCriteriaList([stop_criteria]),\n","                                 temperature=0,\n","                                 max_length=max_length)\n","    \n","    end = time.process_time()\n","    \n","    generated_texts = tokenizer.batch_decode(test_decode, \n","                                             skip_special_tokens=True)\n","    \n","    generated_storage.append(generated_texts)\n","\n","    for j in range(len(generated_texts)):\n","      cur_predicted = generated_texts[j].replace(tokenizer.decode(tokenizer(few_shot[i+j]).input_ids),\"\") # Discard examples and test prompt\n","      cur_predicted = cur_predicted.split(the_stop_word)[0] # Truncate generated in the case that the stopping criteria does not work properly\n","\n","      cur_gold = tokenizer.decode(tokenizer(test[i+j][target], return_tensors='pt').input_ids[0])\n","\n","      predicted_storage.append(cur_predicted)\n","      gold_storage.append(cur_gold)\n","      \n","\n","      print(\"-------------------------\")\n","      print(\"------- Checking --------\")\n","      print(generated_texts[j]) # Generated text with the examples and test prompt\n","      print(f\"Generated : {cur_predicted}\")\n","      # print(f\"{cur_generated.split()}\")\n","      print(f\"Gold : {cur_gold}\")\n","      # print(f\"{cur_gold.split()}\")\n","      print(f\"Match? : {cur_gold == cur_predicted}\")\n","      print(\"-------------------------\")\n","      print(\"                         \")\n","\n","      if cur_gold == cur_predicted:\n","        match += 1 \n","\n","\n","  if device.type == 'cuda':\n","    print('Current cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n","  print(f\"Inference time for batch size {batch_size} : {end-start} , so {(end-start)/batch_size} each\")\n","  print(f\"Current total match for {k} examples : {match}/{total}\")\n","  print(\"                         \")\n","\n","  torch.cuda.empty_cache() # Clear unused memory to prevent out of memory\n","\n","# Save results\n","generated_storage_array = np.array(generated_storage)\n","gold_storage_array = np.array(gold_storage)\n","predicted_storage_array = np.array(predicted_storage)\n","\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Generated {k} Examples - {search_method}', generated_storage_array)\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Gold {k} Examples - {search_method}', gold_storage_array)\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Predicted {k} Examples - {search_method}', predicted_storage_array)\n"],"metadata":{"id":"44Xe2wp8poMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##### SETTINGS #####\n","#@title Search Hyperparameters\n","# number of examples\n","k =  20 #@param {type:\"integer\"}\n","# Don't change batch size\n","batch_size = 1\n","# Max context window\n","if 'gpt2' in model_variant:\n","  max_length = 1024\n","else:\n","  max_length = 2048\n","\n","search_method = search_method\n","##### SETTINGS #####\n","\n","# print(f'Running {model} with {dataset} using {search_method}')\n","\n","few_shot = batch_few_shot_prompt(train, test, num_examples=k, \n","                                 examples_selection=search_method)\n","\n","# Padding has to be on the left for proper text generation because the model uses rightmost token to predict the next token \n","# So padding would result in weird predictions\n","tokenizer.padding_side = \"left\"\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Keep track of how many we actually evaluate with how much is matched.\n","match = 0\n","total = 0\n","\n","# Output storage\n","gold_storage = []\n","generated_storage = []\n","predicted_storage = []\n","\n","for i in range(0, len(test), batch_size):\n","  with torch.no_grad():\n","    few_shot_encoded = tokenizer(few_shot[i:i+batch_size], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length - len(tokenizer(test[i][target])))\n","    if few_shot_encoded.input_ids.shape[1] < (max_length - len(tokenizer(test[i][target]))):\n","      few_shot_encoded = few_shot_encoded.to(device)\n","      total += batch_size\n","    else:\n","      print(\"#############################\")\n","      print(\"####### Batch Skipped #######\")\n","      print(\"#############################\")\n","      print(\"                             \")\n","\n","      continue\n","    \n","    # GPT max output length is 2048\n","    # Beam search results in out of memory error - future work could be to test beam search if more resources are available\n","\n","    start = time.process_time()\n","\n","    test_decode = model.generate(**few_shot_encoded,\n","                                 stopping_criteria=StoppingCriteriaList([stop_criteria]),\n","                                 temperature=0,\n","                                 max_length=max_length)\n","    \n","    end = time.process_time()\n","    \n","    generated_texts = tokenizer.batch_decode(test_decode, \n","                                             skip_special_tokens=True)\n","    \n","    generated_storage.append(generated_texts)\n","\n","    for j in range(len(generated_texts)):\n","      cur_predicted = generated_texts[j].replace(tokenizer.decode(tokenizer(few_shot[i+j]).input_ids),\"\") # Discard examples and test prompt\n","      cur_predicted = cur_predicted.split(the_stop_word)[0] # Truncate generated in the case that the stopping criteria does not work properly\n","\n","      cur_gold = tokenizer.decode(tokenizer(test[i+j][target], return_tensors='pt').input_ids[0])\n","\n","      predicted_storage.append(cur_predicted)\n","      gold_storage.append(cur_gold)\n","      \n","\n","      print(\"-------------------------\")\n","      print(\"------- Checking --------\")\n","      print(generated_texts[j]) # Generated text with the examples and test prompt\n","      print(f\"Generated : {cur_predicted}\")\n","      # print(f\"{cur_generated.split()}\")\n","      print(f\"Gold : {cur_gold}\")\n","      # print(f\"{cur_gold.split()}\")\n","      print(f\"Match? : {cur_gold == cur_predicted}\")\n","      print(\"-------------------------\")\n","      print(\"                         \")\n","\n","      if cur_gold == cur_predicted:\n","        match += 1 \n","\n","\n","  if device.type == 'cuda':\n","    print('Current cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n","  print(f\"Inference time for batch size {batch_size} : {end-start} , so {(end-start)/batch_size} each\")\n","  print(f\"Current total match for {k} examples : {match}/{total}\")\n","  print(\"                         \")\n","\n","  torch.cuda.empty_cache() # Clear unused memory to prevent out of memory\n","\n","# Save results\n","generated_storage_array = np.array(generated_storage)\n","gold_storage_array = np.array(gold_storage)\n","predicted_storage_array = np.array(predicted_storage)\n","\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Generated {k} Examples - {search_method}', generated_storage_array)\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Gold {k} Examples - {search_method}', gold_storage_array)\n","# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Predicted {k} Examples - {search_method}', predicted_storage_array)\n"],"metadata":{"id":"pMpv9BnnppGD"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1RdFLN6qpSW1xFybT3un5qNWHZBOEyUUV","timestamp":1661434472522},{"file_id":"1yDTYROXolVSlhIxwMKou_Kp1WUFTsv9F","timestamp":1661284069582},{"file_id":"16XtIxTtZhHUWE6OdVajPKHPGbxpueGBw","timestamp":1660831190998},{"file_id":"1g2J6k_QU4xKeGPbZX5h_F9kBdL0nITA1","timestamp":1657148821826},{"file_id":"19ThV18Jj0EWTISuo3fijViPdYEXWCiwp","timestamp":1655912910609}],"machine_shape":"hm","authorship_tag":"ABX9TyOkGyrNCSpDHdAn8NGLw/A6"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}