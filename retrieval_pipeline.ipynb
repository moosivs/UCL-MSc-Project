{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJpGHj4ow2DT"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install -U sentence-transformers\n",
        "!pip install rank_bm25\n",
        "\n",
        "from datasets import load_dataset\n",
        "from datasets import get_dataset_config_names\n",
        "from google.colab import drive\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# For debugging torch\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQs368Ehw7MX"
      },
      "outputs": [],
      "source": [
        "# file path\n",
        "%cd /content/drive/My Drive/Colab Notebooks/UCL MSc Project/Data\n",
        "\n",
        "### Settings ###\n",
        "#@title Dataset Setting \n",
        "dataset = 'SCAN' #@param [\"SCAN\", \"COGS\"]\n",
        "\n",
        "print(f'Using {dataset} dataset')\n",
        "\n",
        "\n",
        "data = load_dataset('csv', data_files={'train': f\"./{dataset.lower()}_train.csv\", 'test': f\"./{dataset.lower()}_test.csv\"})\n",
        "\n",
        "train = data['train']\n",
        "test = data['test']\n",
        "import random\n",
        "\n",
        "if dataset == 'SCAN':\n",
        "  input = 'commands'\n",
        "  target = 'actions'\n",
        "\n",
        "if dataset == 'COGS':\n",
        "  train = train.select(range(len(train)-1))\n",
        "  input = 'source'\n",
        "  target = 'target'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9egrP0TrxHM6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList, GPTJForCausalLM\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "##### Select model variant #####\n",
        "#@title Model Setting \n",
        "model_variant = 'gpt-neo-2.7B' #@param [ \"gpt-neo-125M\", \"gpt2-medium\", \"gpt-neo-1.3B\", \"gpt-neo-2.7B\"]\n",
        "print(f\"Running {model_variant}\")\n",
        "\n",
        "if \"gpt2\" in model_variant:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(f\"{model_variant}\")\n",
        "  model = AutoModelForCausalLM.from_pretrained(f\"{model_variant}\")\n",
        "else:\n",
        "  tokenizer = AutoTokenizer.from_pretrained(f\"EleutherAI/{model_variant}\")\n",
        "  model = AutoModelForCausalLM.from_pretrained(f\"EleutherAI/{model_variant}\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48bUuHqX0tYN"
      },
      "outputs": [],
      "source": [
        "# Class to stop the model from generating once it encounters the specified tokens\n",
        "class KeywordsStoppingCriteria(StoppingCriteria):\n",
        "    def __init__(self, keywords_ids:list):\n",
        "        self.keywords = keywords_ids\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        if input_ids[0][-1] in self.keywords:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Set stop tokens\n",
        "if dataset == 'CFQ':\n",
        "  stop_words = ['$']\n",
        "else:\n",
        "  stop_words = ['\\n']\n",
        "\n",
        "stop_ids = [tokenizer.encode(w)[0] for w in stop_words]\n",
        "stop_criteria = KeywordsStoppingCriteria(stop_ids)\n",
        "the_stop_word = stop_words[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI3PfFY0Q1vV"
      },
      "outputs": [],
      "source": [
        "# Create few shot prompt or in-context learning - instructions are optional\n",
        "# Examples are obtained from training data and the test prompt is obtained from test as they have to come from different distributions to test the systematic generalization\n",
        "def batch_few_shot_prompt(train_data: list, test_data: list, num_examples: int, examples_selection='random', include_instruction=False):\n",
        "  batch = []\n",
        "  from scipy.spatial import distance\n",
        "  # N/A as we don't use instructions\n",
        "  # TODO: If we need to do instructions to account for other datasets\n",
        "  instruction = f\"Convert commands to actions based on Simplified version of the CommAI Navigation tasks {the_stop_word}\" if include_instruction else ''\n",
        "  \n",
        "  print(f'running {examples_selection}')\n",
        "  # Pre-calculate cosine similarity\n",
        "  if examples_selection == 'nearest':\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "    sentence_model = SentenceTransformer('sentence-transformers/nli-roberta-base-v2')\n",
        "    test_encode = sentence_model.encode(test_data[input])\n",
        "    train_encode = sentence_model.encode(train_data[input])\n",
        "\n",
        "    from scipy.spatial import distance\n",
        "\n",
        "    cosine_similarity = distance.cdist(test_encode, train_encode, metric='cosine')\n",
        "    cosine_similarity_sorted = np.argsort(cosine_similarity, axis=1)[:, :num_examples]\n",
        "\n",
        "    del sentence_model\n",
        "    torch.cuda.empty_cache()\n",
        "  \n",
        "  # Need to train mepr first: get candidates -> score individual -> train\n",
        "  # Change model path accordingly\n",
        "  if examples_selection == 'mepr':\n",
        "    from transformers import BertTokenizer, BertModel\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model_q = BertModel.from_pretrained(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_q_mepr').to(device)\n",
        "    model_p = BertModel.from_pretrained(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_p_mepr').to(device)\n",
        "\n",
        "    stack = []\n",
        "    chunk_size_test = 300\n",
        "    chunk_size_train = 200\n",
        "\n",
        "    for g in range(0, len(test_data), chunk_size_test):\n",
        "      print(f'g is {g}')\n",
        "      for k in range(0, len(train_data), chunk_size_train):\n",
        "        torch.cuda.empty_cache()\n",
        "        with torch.no_grad():\n",
        "          q = model_q(**bert_tokenizer(test_data[g:g+chunk_size_test][input], return_tensors='pt', padding=True).to(device)).last_hidden_state\n",
        "          p = model_p(**bert_tokenizer(train_data[k:k+chunk_size_train][input], return_tensors='pt', padding=True).to(device)).last_hidden_state\n",
        "\n",
        "        # extract CLS since bert\n",
        "        cls_q = q[:,0]\n",
        "        cls_p = p[:,0]\n",
        "\n",
        "        cosine = distance.cdist(cls_q, cls_p, metric='cosine')\n",
        "        cosine_sorted = np.argsort(cosine, axis=1)[:, :num_examples]\n",
        "        stack.append(cosine_sorted)\n",
        "    \n",
        "    mepr_similarity = np.vstack(stack)\n",
        "\n",
        "    del model_q\n",
        "    del model_p\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  # Need to train fpr individual first: get candidates -> score individual -> train\n",
        "  # Change model path accordingly\n",
        "  if examples_selection == 'individual':\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "    model_q = SentenceTransformer(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_q_individual')\n",
        "    model_p = SentenceTransformer(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_p_individual')\n",
        "\n",
        "    stack = []\n",
        "    chunk_size_test = 300\n",
        "    chunk_size_train = 200\n",
        "\n",
        "    for g in range(0, len(test_data), chunk_size_test):\n",
        "      print(f'g is {g}')\n",
        "      for k in range(0, len(train_data), chunk_size_train):\n",
        "        q = model_q.encode(test_data[g:g+chunk_size_test][input])\n",
        "        p = model_p.encode(train_data[k:k+chunk_size_train][input])\n",
        "\n",
        "        cosine = distance.cdist(q, p, metric='cosine')\n",
        "        cosine_sorted = np.argsort(cosine, axis=1)[:, :num_examples]\n",
        "        stack.append(cosine_sorted)\n",
        "    \n",
        "    individual_similarity = np.vstack(stack)\n",
        "\n",
        "    del model_q\n",
        "    del model_p\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  # Need to train fpr pairwise first: get candidates -> score pair -> train\n",
        "  # Change model path accordingly\n",
        "  if examples_selection == 'pairwise':\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "    model_q = SentenceTransformer(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_q_pairwise')\n",
        "    model_p = SentenceTransformer(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/dense retriever/{dataset}_E_p_pairwise')\n",
        "\n",
        "    stack = []\n",
        "    chunk_size_test = 300\n",
        "    chunk_size_train = 200\n",
        "\n",
        "    for g in range(0, len(test_data), chunk_size_test):\n",
        "      print(f'g is {g}')\n",
        "      for k in range(0, len(train_data), chunk_size_train):\n",
        "        q = model_q.encode(test_data[g:g+chunk_size_test][input])\n",
        "        p = model_p.encode(train_data[k:k+chunk_size_train][input])\n",
        "\n",
        "        cosine = distance.cdist(q, p, metric='cosine')\n",
        "        cosine_sorted = np.argsort(cosine, axis=1)[:, :num_examples]\n",
        "        stack.append(cosine_sorted)\n",
        "    \n",
        "    pairwise_similarity = np.vstack(stack)\n",
        "\n",
        "    del model_q\n",
        "    del model_p\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  # Assign examples for each of the test prompt\n",
        "  for i in range(len(test_data)):\n",
        "    print(f'generating test prompt i is {i}')\n",
        "    few_shot_prompt = ''\n",
        "    # Get training examples indices\n",
        "    if examples_selection == 'random':\n",
        "      train_examples_pos = np.random.choice(len(train_data) , num_examples, replace=False)\n",
        "    if examples_selection == 'nearest':\n",
        "      train_examples_pos = cosine_similarity_sorted[i, :]\n",
        "    if examples_selection == 'medpr':\n",
        "      train_examples_pos = mepr_similarity[i,:]\n",
        "    if examples_selection == 'individual':\n",
        "      train_examples_pos = individual_similarity[i, :]\n",
        "    if examples_selection == 'pairwise':\n",
        "      train_examples_pos = pairwise_similarity[i, :]\n",
        "      \n",
        "\n",
        "    few_shot_prompt += instruction\n",
        "\n",
        "    for j in train_examples_pos.tolist():\n",
        "      few_shot_prompt += (f\"{input}:\" + train_data[j][input] + ' ' + f\"{target}:\" + train_data[j][target] + the_stop_word)\n",
        "\n",
        "    few_shot_prompt += (f\"{input}:\" + test_data[i][input] + ' ' + f\"{target}:\")\n",
        "\n",
        "    batch.append(few_shot_prompt)\n",
        "  \n",
        "  # Sanity check\n",
        "  print(\"++++++++++++++++++++++++++\")\n",
        "  print(\"++++++ Sanity Check ++++++\")\n",
        "  print(batch[np.random.randint(0, len(test_data))])\n",
        "  print(\"++++++++++++++++++++++++++\")\n",
        "  print(\"                          \")\n",
        "\n",
        "  return batch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nf4OHdrQrvg"
      },
      "outputs": [],
      "source": [
        "##### SETTINGS #####\n",
        "#@title Search Hyperparameters\n",
        "# number of examples\n",
        "k =  1 #@param {type:\"integer\"}\n",
        "# Don't change batch size\n",
        "batch_size = 1\n",
        "# Max context window\n",
        "if 'gpt2' in model_variant:\n",
        "  max_length = 1024\n",
        "else:\n",
        "  max_length = 2048\n",
        "\n",
        "search_method = 'random' #@param [\"random\", \"nearest\", \"mepr\", \"individual\", \"pairwise\"]\n",
        "##### SETTINGS #####\n",
        "\n",
        "# print(f'Running {model} with {dataset} using {search_method}')\n",
        "\n",
        "few_shot = batch_few_shot_prompt(train, test, num_examples=k, \n",
        "                                 examples_selection=search_method)\n",
        "\n",
        "# Padding has to be on the left for proper text generation because the model uses rightmost token to predict the next token \n",
        "# So padding would result in weird predictions\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Keep track of how many we actually evaluate with how much is matched.\n",
        "match = 0\n",
        "total = 0\n",
        "\n",
        "# Output storage\n",
        "gold_storage = []\n",
        "generated_storage = []\n",
        "predicted_storage = []\n",
        "\n",
        "for i in range(0, len(test), batch_size):\n",
        "  with torch.no_grad():\n",
        "    few_shot_encoded = tokenizer(few_shot[i:i+batch_size], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length - len(tokenizer(test[i][target])))\n",
        "    if few_shot_encoded.input_ids.shape[1] < (max_length - len(tokenizer(test[i][target]))):\n",
        "      few_shot_encoded = few_shot_encoded.to(device)\n",
        "      total += batch_size\n",
        "    else:\n",
        "      print(\"#############################\")\n",
        "      print(\"####### Batch Skipped #######\")\n",
        "      print(\"#############################\")\n",
        "      print(\"                             \")\n",
        "\n",
        "      continue\n",
        "    \n",
        "    # GPT max output length is 2048\n",
        "    # Beam search results in out of memory error - future work could be to test beam search if more resources are available\n",
        "\n",
        "    start = time.process_time()\n",
        "\n",
        "    test_decode = model.generate(**few_shot_encoded,\n",
        "                                 stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
        "                                 temperature=0,\n",
        "                                 max_length=max_length)\n",
        "    \n",
        "    end = time.process_time()\n",
        "    \n",
        "    generated_texts = tokenizer.batch_decode(test_decode, \n",
        "                                             skip_special_tokens=True)\n",
        "    \n",
        "    generated_storage.append(generated_texts)\n",
        "\n",
        "    for j in range(len(generated_texts)):\n",
        "      cur_predicted = generated_texts[j].replace(tokenizer.decode(tokenizer(few_shot[i+j]).input_ids),\"\") # Discard examples and test prompt\n",
        "      cur_predicted = cur_predicted.split(the_stop_word)[0] # Truncate generated in the case that the stopping criteria does not work properly\n",
        "\n",
        "      cur_gold = tokenizer.decode(tokenizer(test[i+j][target], return_tensors='pt').input_ids[0])\n",
        "\n",
        "      predicted_storage.append(cur_predicted)\n",
        "      gold_storage.append(cur_gold)\n",
        "      \n",
        "\n",
        "      print(\"-------------------------\")\n",
        "      print(\"------- Checking --------\")\n",
        "      print(generated_texts[j]) # Generated text with the examples and test prompt\n",
        "      print(f\"Generated : {cur_predicted}\")\n",
        "      # print(f\"{cur_generated.split()}\")\n",
        "      print(f\"Gold : {cur_gold}\")\n",
        "      # print(f\"{cur_gold.split()}\")\n",
        "      print(f\"Match? : {cur_gold == cur_predicted}\")\n",
        "      print(\"-------------------------\")\n",
        "      print(\"                         \")\n",
        "\n",
        "      if cur_gold == cur_predicted:\n",
        "        match += 1 \n",
        "\n",
        "\n",
        "  if device.type == 'cuda':\n",
        "    print('Current cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "  print(f\"Inference time for batch size {batch_size} : {end-start} , so {(end-start)/batch_size} each\")\n",
        "  print(f\"Current total match for {k} examples : {match}/{total}\")\n",
        "  print(\"                         \")\n",
        "\n",
        "  torch.cuda.empty_cache() # Clear unused memory to prevent out of memory\n",
        "\n",
        "# Save results\n",
        "generated_storage_array = np.array(generated_storage)\n",
        "gold_storage_array = np.array(gold_storage)\n",
        "predicted_storage_array = np.array(predicted_storage)\n",
        "\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Generated {k} Examples - {search_method}', generated_storage_array)\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Gold {k} Examples - {search_method}', gold_storage_array)\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Predicted {k} Examples - {search_method}', predicted_storage_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### SETTINGS #####\n",
        "#@title Search Hyperparameters\n",
        "# number of examples\n",
        "k =  5 #@param {type:\"integer\"}\n",
        "# Don't change batch size\n",
        "batch_size = 1\n",
        "# Max context window\n",
        "if 'gpt2' in model_variant:\n",
        "  max_length = 1024\n",
        "else:\n",
        "  max_length = 2048\n",
        "\n",
        "search_method = search_method\n",
        "##### SETTINGS #####\n",
        "\n",
        "# print(f'Running {model} with {dataset} using {search_method}')\n",
        "\n",
        "few_shot = batch_few_shot_prompt(train, test, num_examples=k, \n",
        "                                 examples_selection=search_method)\n",
        "\n",
        "# Padding has to be on the left for proper text generation because the model uses rightmost token to predict the next token \n",
        "# So padding would result in weird predictions\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Keep track of how many we actually evaluate with how much is matched.\n",
        "match = 0\n",
        "total = 0\n",
        "\n",
        "# Output storage\n",
        "gold_storage = []\n",
        "generated_storage = []\n",
        "predicted_storage = []\n",
        "\n",
        "for i in range(0, len(test), batch_size):\n",
        "  with torch.no_grad():\n",
        "    few_shot_encoded = tokenizer(few_shot[i:i+batch_size], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length - len(tokenizer(test[i][target])))\n",
        "    if few_shot_encoded.input_ids.shape[1] < (max_length - len(tokenizer(test[i][target]))):\n",
        "      few_shot_encoded = few_shot_encoded.to(device)\n",
        "      total += batch_size\n",
        "    else:\n",
        "      print(\"#############################\")\n",
        "      print(\"####### Batch Skipped #######\")\n",
        "      print(\"#############################\")\n",
        "      print(\"                             \")\n",
        "\n",
        "      continue\n",
        "    \n",
        "    # GPT max output length is 2048\n",
        "    # Beam search results in out of memory error - future work could be to test beam search if more resources are available\n",
        "\n",
        "    start = time.process_time()\n",
        "\n",
        "    test_decode = model.generate(**few_shot_encoded,\n",
        "                                 stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
        "                                 temperature=0,\n",
        "                                 max_length=max_length)\n",
        "    \n",
        "    end = time.process_time()\n",
        "    \n",
        "    generated_texts = tokenizer.batch_decode(test_decode, \n",
        "                                             skip_special_tokens=True)\n",
        "    \n",
        "    generated_storage.append(generated_texts)\n",
        "\n",
        "    for j in range(len(generated_texts)):\n",
        "      cur_predicted = generated_texts[j].replace(tokenizer.decode(tokenizer(few_shot[i+j]).input_ids),\"\") # Discard examples and test prompt\n",
        "      cur_predicted = cur_predicted.split(the_stop_word)[0] # Truncate generated in the case that the stopping criteria does not work properly\n",
        "\n",
        "      cur_gold = tokenizer.decode(tokenizer(test[i+j][target], return_tensors='pt').input_ids[0])\n",
        "\n",
        "      predicted_storage.append(cur_predicted)\n",
        "      gold_storage.append(cur_gold)\n",
        "      \n",
        "\n",
        "      print(\"-------------------------\")\n",
        "      print(\"------- Checking --------\")\n",
        "      print(generated_texts[j]) # Generated text with the examples and test prompt\n",
        "      print(f\"Generated : {cur_predicted}\")\n",
        "      # print(f\"{cur_generated.split()}\")\n",
        "      print(f\"Gold : {cur_gold}\")\n",
        "      # print(f\"{cur_gold.split()}\")\n",
        "      print(f\"Match? : {cur_gold == cur_predicted}\")\n",
        "      print(\"-------------------------\")\n",
        "      print(\"                         \")\n",
        "\n",
        "      if cur_gold == cur_predicted:\n",
        "        match += 1 \n",
        "\n",
        "\n",
        "  if device.type == 'cuda':\n",
        "    print('Current cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "  print(f\"Inference time for batch size {batch_size} : {end-start} , so {(end-start)/batch_size} each\")\n",
        "  print(f\"Current total match for {k} examples : {match}/{total}\")\n",
        "  print(\"                         \")\n",
        "\n",
        "  torch.cuda.empty_cache() # Clear unused memory to prevent out of memory\n",
        "\n",
        "# Save results\n",
        "generated_storage_array = np.array(generated_storage)\n",
        "gold_storage_array = np.array(gold_storage)\n",
        "predicted_storage_array = np.array(predicted_storage)\n",
        "\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Generated {k} Examples - {search_method}', generated_storage_array)\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Gold {k} Examples - {search_method}', gold_storage_array)\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Predicted {k} Examples - {search_method}', predicted_storage_array)\n"
      ],
      "metadata": {
        "id": "IsMzupWQpnBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### SETTINGS #####\n",
        "#@title Search Hyperparameters\n",
        "# number of examples\n",
        "k =  10 #@param {type:\"integer\"}\n",
        "# Don't change batch size\n",
        "batch_size = 1\n",
        "# Max context window\n",
        "if 'gpt2' in model_variant:\n",
        "  max_length = 1024\n",
        "else:\n",
        "  max_length = 2048\n",
        "\n",
        "search_method = search_method\n",
        "##### SETTINGS #####\n",
        "\n",
        "# print(f'Running {model} with {dataset} using {search_method}')\n",
        "\n",
        "few_shot = batch_few_shot_prompt(train, test, num_examples=k, \n",
        "                                 examples_selection=search_method)\n",
        "\n",
        "# Padding has to be on the left for proper text generation because the model uses rightmost token to predict the next token \n",
        "# So padding would result in weird predictions\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Keep track of how many we actually evaluate with how much is matched.\n",
        "match = 0\n",
        "total = 0\n",
        "\n",
        "# Output storage\n",
        "gold_storage = []\n",
        "generated_storage = []\n",
        "predicted_storage = []\n",
        "\n",
        "for i in range(0, len(test), batch_size):\n",
        "  with torch.no_grad():\n",
        "    few_shot_encoded = tokenizer(few_shot[i:i+batch_size], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length - len(tokenizer(test[i][target])))\n",
        "    if few_shot_encoded.input_ids.shape[1] < (max_length - len(tokenizer(test[i][target]))):\n",
        "      few_shot_encoded = few_shot_encoded.to(device)\n",
        "      total += batch_size\n",
        "    else:\n",
        "      print(\"#############################\")\n",
        "      print(\"####### Batch Skipped #######\")\n",
        "      print(\"#############################\")\n",
        "      print(\"                             \")\n",
        "\n",
        "      continue\n",
        "    \n",
        "    # GPT max output length is 2048\n",
        "    # Beam search results in out of memory error - future work could be to test beam search if more resources are available\n",
        "\n",
        "    start = time.process_time()\n",
        "\n",
        "    test_decode = model.generate(**few_shot_encoded,\n",
        "                                 stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
        "                                 temperature=0,\n",
        "                                 max_length=max_length)\n",
        "    \n",
        "    end = time.process_time()\n",
        "    \n",
        "    generated_texts = tokenizer.batch_decode(test_decode, \n",
        "                                             skip_special_tokens=True)\n",
        "    \n",
        "    generated_storage.append(generated_texts)\n",
        "\n",
        "    for j in range(len(generated_texts)):\n",
        "      cur_predicted = generated_texts[j].replace(tokenizer.decode(tokenizer(few_shot[i+j]).input_ids),\"\") # Discard examples and test prompt\n",
        "      cur_predicted = cur_predicted.split(the_stop_word)[0] # Truncate generated in the case that the stopping criteria does not work properly\n",
        "\n",
        "      cur_gold = tokenizer.decode(tokenizer(test[i+j][target], return_tensors='pt').input_ids[0])\n",
        "\n",
        "      predicted_storage.append(cur_predicted)\n",
        "      gold_storage.append(cur_gold)\n",
        "      \n",
        "\n",
        "      print(\"-------------------------\")\n",
        "      print(\"------- Checking --------\")\n",
        "      print(generated_texts[j]) # Generated text with the examples and test prompt\n",
        "      print(f\"Generated : {cur_predicted}\")\n",
        "      # print(f\"{cur_generated.split()}\")\n",
        "      print(f\"Gold : {cur_gold}\")\n",
        "      # print(f\"{cur_gold.split()}\")\n",
        "      print(f\"Match? : {cur_gold == cur_predicted}\")\n",
        "      print(\"-------------------------\")\n",
        "      print(\"                         \")\n",
        "\n",
        "      if cur_gold == cur_predicted:\n",
        "        match += 1 \n",
        "\n",
        "\n",
        "  if device.type == 'cuda':\n",
        "    print('Current cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "  print(f\"Inference time for batch size {batch_size} : {end-start} , so {(end-start)/batch_size} each\")\n",
        "  print(f\"Current total match for {k} examples : {match}/{total}\")\n",
        "  print(\"                         \")\n",
        "\n",
        "  torch.cuda.empty_cache() # Clear unused memory to prevent out of memory\n",
        "\n",
        "# Save results\n",
        "generated_storage_array = np.array(generated_storage)\n",
        "gold_storage_array = np.array(gold_storage)\n",
        "predicted_storage_array = np.array(predicted_storage)\n",
        "\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Generated {k} Examples - {search_method}', generated_storage_array)\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Gold {k} Examples - {search_method}', gold_storage_array)\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Predicted {k} Examples - {search_method}', predicted_storage_array)\n"
      ],
      "metadata": {
        "id": "44Xe2wp8poMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### SETTINGS #####\n",
        "#@title Search Hyperparameters\n",
        "# number of examples\n",
        "k =  20 #@param {type:\"integer\"}\n",
        "# Don't change batch size\n",
        "batch_size = 1\n",
        "# Max context window\n",
        "if 'gpt2' in model_variant:\n",
        "  max_length = 1024\n",
        "else:\n",
        "  max_length = 2048\n",
        "\n",
        "search_method = search_method\n",
        "##### SETTINGS #####\n",
        "\n",
        "# print(f'Running {model} with {dataset} using {search_method}')\n",
        "\n",
        "few_shot = batch_few_shot_prompt(train, test, num_examples=k, \n",
        "                                 examples_selection=search_method)\n",
        "\n",
        "# Padding has to be on the left for proper text generation because the model uses rightmost token to predict the next token \n",
        "# So padding would result in weird predictions\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Keep track of how many we actually evaluate with how much is matched.\n",
        "match = 0\n",
        "total = 0\n",
        "\n",
        "# Output storage\n",
        "gold_storage = []\n",
        "generated_storage = []\n",
        "predicted_storage = []\n",
        "\n",
        "for i in range(0, len(test), batch_size):\n",
        "  with torch.no_grad():\n",
        "    few_shot_encoded = tokenizer(few_shot[i:i+batch_size], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length - len(tokenizer(test[i][target])))\n",
        "    if few_shot_encoded.input_ids.shape[1] < (max_length - len(tokenizer(test[i][target]))):\n",
        "      few_shot_encoded = few_shot_encoded.to(device)\n",
        "      total += batch_size\n",
        "    else:\n",
        "      print(\"#############################\")\n",
        "      print(\"####### Batch Skipped #######\")\n",
        "      print(\"#############################\")\n",
        "      print(\"                             \")\n",
        "\n",
        "      continue\n",
        "    \n",
        "    # GPT max output length is 2048\n",
        "    # Beam search results in out of memory error - future work could be to test beam search if more resources are available\n",
        "\n",
        "    start = time.process_time()\n",
        "\n",
        "    test_decode = model.generate(**few_shot_encoded,\n",
        "                                 stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
        "                                 temperature=0,\n",
        "                                 max_length=max_length)\n",
        "    \n",
        "    end = time.process_time()\n",
        "    \n",
        "    generated_texts = tokenizer.batch_decode(test_decode, \n",
        "                                             skip_special_tokens=True)\n",
        "    \n",
        "    generated_storage.append(generated_texts)\n",
        "\n",
        "    for j in range(len(generated_texts)):\n",
        "      cur_predicted = generated_texts[j].replace(tokenizer.decode(tokenizer(few_shot[i+j]).input_ids),\"\") # Discard examples and test prompt\n",
        "      cur_predicted = cur_predicted.split(the_stop_word)[0] # Truncate generated in the case that the stopping criteria does not work properly\n",
        "\n",
        "      cur_gold = tokenizer.decode(tokenizer(test[i+j][target], return_tensors='pt').input_ids[0])\n",
        "\n",
        "      predicted_storage.append(cur_predicted)\n",
        "      gold_storage.append(cur_gold)\n",
        "      \n",
        "\n",
        "      print(\"-------------------------\")\n",
        "      print(\"------- Checking --------\")\n",
        "      print(generated_texts[j]) # Generated text with the examples and test prompt\n",
        "      print(f\"Generated : {cur_predicted}\")\n",
        "      # print(f\"{cur_generated.split()}\")\n",
        "      print(f\"Gold : {cur_gold}\")\n",
        "      # print(f\"{cur_gold.split()}\")\n",
        "      print(f\"Match? : {cur_gold == cur_predicted}\")\n",
        "      print(\"-------------------------\")\n",
        "      print(\"                         \")\n",
        "\n",
        "      if cur_gold == cur_predicted:\n",
        "        match += 1 \n",
        "\n",
        "\n",
        "  if device.type == 'cuda':\n",
        "    print('Current cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "  print(f\"Inference time for batch size {batch_size} : {end-start} , so {(end-start)/batch_size} each\")\n",
        "  print(f\"Current total match for {k} examples : {match}/{total}\")\n",
        "  print(\"                         \")\n",
        "\n",
        "  torch.cuda.empty_cache() # Clear unused memory to prevent out of memory\n",
        "\n",
        "# Save results\n",
        "generated_storage_array = np.array(generated_storage)\n",
        "gold_storage_array = np.array(gold_storage)\n",
        "predicted_storage_array = np.array(predicted_storage)\n",
        "\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Generated {k} Examples - {search_method}', generated_storage_array)\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Gold {k} Examples - {search_method}', gold_storage_array)\n",
        "# np.save(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Results/{dataset} {model_variant.upper()} Predicted {k} Examples - {search_method}', predicted_storage_array)\n"
      ],
      "metadata": {
        "id": "pMpv9BnnppGD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}