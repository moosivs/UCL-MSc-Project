{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AHUr7wCetcy0"},"outputs":[],"source":["!pip install datasets\n","!pip install transformers\n","!pip install -U sentence-transformers\n","\n","from datasets import load_dataset\n","from datasets import get_dataset_config_names\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","import time\n","  import ast\n","\n","# For debugging torch\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFesQAcQtgPY"},"outputs":[],"source":["### Settings ###\n","#@title Dataset Setting \n","dataset = 'COGS' #@param [\"SCAN\", \"CFQ\", \"COGS\"]\n","\n","print(f'Using {dataset} dataset')\n","\n","%cd /content/drive/My Drive/Colab Notebooks/UCL MSc Project/Data\n","\n","data = load_dataset('csv', data_files={'train': f\"./{dataset.lower()}_train.csv\", 'test': f\"./{dataset.lower()}_test.csv\"})\n","\n","train = data['train']\n","test = data['test']\n","\n","if dataset == 'SCAN':\n","  input = 'commands'\n","  target = 'actions'\n","\n","if dataset == 'COGS':\n","  train = train.select(range(len(train)-1))\n","  input = 'source'\n","  target = 'target'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ol28_Sy_tiJ6"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList, GPTJForCausalLM\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","##### Select model variant #####\n","model_variant = 'gpt-neo-2.7B'\n","print(f\"Running {model_variant}\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(f\"EleutherAI/{model_variant}\")\n","model = AutoModelForCausalLM.from_pretrained(f\"EleutherAI/{model_variant}\")\n","model.to(device)\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n","\n","# model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", low_cpu_mem_usage=True)\n","# model.to(device)\n","# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n","\n","# # The tokenizer does not have padding token\n","# if tokenizer.pad_token is None:\n","#     tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","# # Need to resize the model vocab size as we have added an extra token for padding\n","# model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","source":["# scoring\n","def get_prob_single(candidates_index:list, train_data, test_data, dataset, scoring_model, return_index=True):\n","  # Per test point, score from the training data\n","\n","  desired_tokens = tokenizer(test_data[target]).input_ids\n","  desired_length = len(desired_tokens)\n","\n","  # print(desired_length)\n","\n","  cum_prob = []\n","  index_list = []\n","  for index in candidates_index:\n","    torch.cuda.empty_cache()\n","\n","    index = int(index)\n","    scoring_prompt = ''\n","    scoring_prompt += (f\"{input}:\" + train_data[index][input] + ' ' + f\"{target}:\" + train_data[index][target] + '\\n')\n","    scoring_prompt += (f\"{input}:\" + test_data[input] + ' ' + f\"{target}:{test_data[target]}\")\n","\n","\n","    tokenized_scoring_prompt = tokenizer(scoring_prompt, return_tensors='pt').input_ids[0].to(device)\n","\n","    with torch.no_grad():\n","      check_test = scoring_model(tokenized_scoring_prompt, labels=tokenized_scoring_prompt)\n","\n","    prob = torch.softmax(check_test.logits[-desired_length:], dim=1)\n","    cur_prob = 0\n","    \n","    torch.cuda.empty_cache()\n","    \n","    for j in range(prob.shape[0]):\n","      cur_prob += torch.log(prob[j, desired_tokens[j]])\n","\n","    cum_prob.append(int(cur_prob.cpu().numpy()))\n","    index_list.append(index)\n","\n","  return zip(cum_prob, index_list)\n","\n","\n"],"metadata":{"id":"TJqXLQRdnrl5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bjmfco4tq7_"},"outputs":[],"source":["# scoring\n","def get_prob_pair(candidates_index:list, train_data, test_data, dataset, scoring_model, return_index=True):\n","  # Per test point, score from the training data\n","\n","  desired_tokens = tokenizer(test_data[target]).input_ids\n","  desired_length = len(desired_tokens)\n","\n","  # print(desired_length)\n","\n","  cum_prob = []\n","  index_list = []\n","  for i in range(len(candidates_index)):\n","    for j in range(i, len(candidates_index)):\n","\n","      torch.cuda.empty_cache()\n","\n","      scoring_prompt = ''\n","      scoring_prompt += (f\"{input}:\" + train_data[int(candidates_index[i])][input] + ' ' + f\"{target}:\" + train_data[int(candidates_index[i])][target] + '\\n')\n","      scoring_prompt += (f\"{input}:\" + train_data[int(candidates_index[j])][input] + ' ' + f\"{target}:\" + train_data[int(candidates_index[j])][target] + '\\n')\n","      scoring_prompt += (f\"{input}:\" + test_data[input] + ' ' + f\"{target}:{test_data[target]}\")\n","\n","\n","      tokenized_scoring_prompt = tokenizer(scoring_prompt, return_tensors='pt').input_ids[0].to(device)\n","\n","      with torch.no_grad():\n","        check_test = scoring_model(tokenized_scoring_prompt, labels=tokenized_scoring_prompt)\n","\n","      prob = torch.softmax(check_test.logits[-desired_length:], dim=1)\n","      cur_prob = 0\n","      \n","      torch.cuda.empty_cache()\n","      \n","      for j in range(prob.shape[0]):\n","        cur_prob += torch.log(prob[j, desired_tokens[j]])\n","\n","      cum_prob.append(int(cur_prob.cpu().numpy()))\n","  \n","      index_list.append(set(candidates_index[i], candidates_index[j]))\n","\n","    return zip(cum_prob, index_list)\n","\n","\n"]},{"cell_type":"code","source":["# Get top-p and bottom-p samples\n","print(f'Running for {dataset}')\n","\n","top_five_storage = []\n","bottom_five_storage = []\n","\n","index_list = np.load(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Candidate Examples Index/{dataset} Top 75 SBERT Index.npy')\n","\n","anchor_storage = []\n","top_five_storage = []\n","bottom_five_storage = []\n","\n","\n","for i in range(len(test)):\n","  # get single score and index per test point\n","  single = np.array(list(get_prob_single(index_list[i], train, test[i], dataset, model))) # (Score, Index) Single\n","  single_sorted_score= single[single[:, 0].argsort()][::-1] # Descending order\n","  single_top_five_score = list(single_sorted_score[:5, 0]) # 1st 2nd 3rd... get score\n","  single_top_five_index = list(single_sorted_score[:5, 1]) # 1st 2nd 3rd... get index\n","\n","\n","  # get pairwise score and index per test point\n","  pair = np.array(list(get_prob_pair(index_list[i], train, test[i], dataset, model))) # (Score, set(Index_1, Index_2)) Pair\n","\n","  # search through each anchor in top single scoring\n","  for anchor_index, anchor_score in (single_top_five_index, single_top_five_score):\n","    pos_index_list = []\n","    neg_index_list = []\n","    \n","    cur_anchor.append(anchor_index)\n","    for p in range(len(pair)):\n","      if anchor_index in pair[p][1]:\n","        if anchor_score > pair[p][0]:\n","          pos_index_list.append(p)\n","        else:\n","          neg_index_list.append(p)\n","\n","    \n","    pos = pair[pos_index_list, :]\n","    pos_sorted_score = pos[pos[:, 0].argsort()][::-1] # Descending order\n","    pos_top_five_index = []\n","\n","    for tmp in range(5):\n","      pos_top_five_index.append(pos_sorted_score[tmp, 1].intersection(anchor_index).pop())\n","\n","    neg = pair[neg_index_list, :]\n","    neg_sorted_score = neg[neg[:, 0].argsort()][::-1] # Descending order\n","    neg_top_five_index = []\n","\n","    for tmp in range(5):\n","      neg_top_five_index.append(neg_sorted_score[-tmp, 1].intersection(anchor_index).pop())\n","\n","\n","    anchor_storage.append(anchor_index)\n","    top_five_storage.append(pos_top_five_index)\n","    bottom_five_storage.append(neg_top_five_index)\n","\n","data = {'anchor': anchor_storage ,'top_five': top_five_storage, 'bottom_five': bottom_five_storage}\n","\n","df = pd.DataFrame(data)\n","\n","df.to_csv(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Top and Bottom Five/{dataset} pair', index=False)\n","print(f'{dataset} Final')"],"metadata":{"id":"TG_DC8BLJNop"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOOEnHUQJsijxAqpqIOoPTA"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}