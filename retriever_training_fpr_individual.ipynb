{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dTjeemsRFqxi"},"outputs":[],"source":["!pip install datasets\n","!pip install transformers\n","!pip install -U sentence-transformers\n","!pip install rank_bm25\n","\n","from datasets import load_dataset\n","from datasets import get_dataset_config_names\n","from sentence_transformers import SentenceTransformer\n","\n","import torch\n","import numpy as np\n","import time\n","from rank_bm25 import BM25Okapi\n","\n","# For debugging torch\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mN0foCu7Gdbj"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using {device}')\n","from transformers import AutoTokenizer, AutoModel\n","\n","tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/nli-roberta-base-v2')\n","model_q = AutoModel.from_pretrained('sentence-transformers/nli-roberta-base-v2').to(device)\n","model_p = AutoModel.from_pretrained('sentence-transformers/nli-roberta-base-v2').to(device)\n","\n","#Mean Pooling - Take attention mask into account for correct averaging\n","def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-3KhwQDFYlrY"},"outputs":[],"source":["### Settings ###\n","#@title Dataset Setting \n","dataset = 'COGS' #@param [\"SCAN\", \"CFQ\", \"COGS\"]\n","\n","print(f'Using {dataset} dataset')\n","\n","%cd /content/drive/My Drive/Colab Notebooks/UCL MSc Project/Data\n","\n","data = load_dataset('csv', data_files={'train': f\"./{dataset.lower()}_train.csv\", 'test': f\"./{dataset.lower()}_test.csv\"})\n","\n","train = data['train']\n","test = data['test']\n","\n","if dataset == 'SCAN':\n","  input = 'commands'\n","  target = 'actions'\n","\n","if dataset == 'COGS':\n","  input = 'source'\n","  target = 'target'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBJdRmr0fW8u"},"outputs":[],"source":["def MC_pair_loss(q, p_pos, p_neg:list, similarity_metric='dot'):\n","  # CLS hidden state of each for each of the representation\n","  # one query, one p positive, many p negatives\n","\n","  q = q[0]\n","  p_pos = p_pos[0]\n","  if similarity_metric == 'dot':   \n","    # Normalise the vectors to prevent arithmetic overflow with the exponentials\n","    p_pos = torch.nn.functional.normalize(p_pos, dim=0)\n","    q = torch.nn.functional.normalize(q, dim=0)\n","\n","    numerator = torch.exp(torch.dot(q, p_pos))\n","\n","    denominator = 0\n","    for i in range(len(p_neg)):\n","      denominator += torch.exp(torch.dot(q, torch.nn.functional.normalize(p_neg[i], dim=0)))\n","    denominator += torch.exp(torch.dot(q, p_pos))\n","\n","  loss = -torch.log(numerator/denominator)\n","\n","  return loss\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjSvYKfuGtcq"},"outputs":[],"source":["def format_pos_neg(LIST, B=50):\n","\n","  import random\n","  positive_list = []\n","  negative_list = []\n","  for i in range(len(LIST)):\n","    negative = []\n","    # Sample a positive from top k of the instance\n","    positive_list.append(random.choice(LIST[i][0]))\n","    # Sample a negative from bottom k of the instance\n","    negative.append(random.choice(LIST[i][1]))\n","\n","    for j in range(B-1):\n","      # Sample an instance that is not the current instance\n","      while True:\n","        pick = random.randint((i//B) * B, ((i//B + 1) * B) -1)\n","        if i != pick:\n","          break\n","\n","      # Sample a negative from each of top and bottom k from the picked instance\n","      # negative.append(random.choice(LIST[pick][0]))\n","      negative.append(random.choice(LIST[pick][1]))\n","    \n","    negative_list.append(negative)\n","  \n","  return positive_list, negative_list\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmQQRcfOVIvq"},"outputs":[],"source":["# Training loop\n","def train_individual():\n","  import pandas as pd\n","  import ast\n","  # Load top-p and bottom-p data\n","\n","  batch_size = 50\n","  a, b = format_pos_neg(pd.read_csv(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Top and Bottom Five/{dataset}_individual', converters={0:ast.literal_eval, 1:ast.literal_eval}).to_numpy(),\n","                        B=batch_size)\n","\n","  optimizer =  torch.optim.Adam(list(model_q.parameters()) + list(model_p.parameters()), lr=10**-4)\n","  scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor =1, end_factor=0.1, total_iters=10)\n","\n","\n","  batches = len(a) // batch_size\n","\n","  epoch_loss_storage = []\n","\n","  for epoch in range(30):\n","    # Per epoch\n","    epoch_loss = 0\n","    for batch in range(batches):\n","      # Per batch\n","      loss = 0\n","      torch.cuda.empty_cache()\n","      optimizer.zero_grad()\n","      for i in range(batch*batch_size, (batch+1)*batch_size):\n","        # Per data point\n","        q_encoded = tokenizer(test[i][input], return_tensors='pt').to(device)\n","        q_output = model_q(**q_encoded)\n","        q_rep = mean_pooling(q_output, q_encoded['attention_mask'])\n","        \n","        p_encoded_positive = tokenizer(train[a[i]][input], return_tensors='pt').to(device)\n","        p_output_positive = model_p(**p_encoded_positive)\n","        p_rep_positive = mean_pooling(p_output_positive, p_encoded_positive['attention_mask'])\n","\n","        p_encoded_negative = tokenizer(train.select(b[i])[input], return_tensors='pt', padding=True).to(device)\n","        p_output_negative = model_p(**p_encoded_negative)\n","        p_rep_negative = mean_pooling(p_output_negative, p_encoded_negative['attention_mask'])\n","\n","        loss += MC_pair_loss(q_rep, p_rep_positive, p_rep_negative, similarity_metric='dot') # loss calculation \n","      loss /= batch_size\n","      loss.backward()\n","      optimizer.step()\n","      epoch_loss += loss.item()\n","      print(f'Loss per {batch}batch {loss.item()} at epoch {epoch}')\n","\n","    \n","    epoch_loss_storage.append(epoch_loss/batches)\n","    print(f'epoch {epoch } loss is {epoch_loss/batches}')\n","    scheduler.step()\n","      \n","    model_q.save_pretrained(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Dense Retriever/{dataset}_E_q_individual')\n","    model_p.save_pretrained(f'/content/drive/My Drive/Colab Notebooks/UCL MSc Project/Dense Retriever/{dataset}_E_p_individual')   "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyOUyozM2mAtEcUROBj0TPNX"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}